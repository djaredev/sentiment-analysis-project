{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import contractions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = pd.read_csv(\"./input/sentiment140.csv\", encoding=\"ISO-8859-1\", names=columns)\n",
    "df.drop([\"id\", \"date\", \"flag\", \"user\"], axis=\"columns\", inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    if not text == \"\":\n",
    "        text = text.lower()  # Convertir minusculas todo el texto\n",
    "        text = re.sub(\n",
    "            r\"@[\\S]+\", \"\", text\n",
    "        )  # Eliminar los nombres de usuarios con @ mencionados\n",
    "        text = re.sub(\n",
    "            r\"((www\\.[\\S]+)|([https]+://[\\S]+))\", \"\", text\n",
    "        )  # Eliminar las urls mencionadas\n",
    "        text = re.sub(\n",
    "            r\"^\\s+|\\s+$|\\s+(?=\\s)\", \"\", text\n",
    "        )  # Eliminar espacios en blanco extras\n",
    "        text = contractions.fix(text)  # type: ignore # Expandir las contracciones\n",
    "\n",
    "        text = re.sub(\n",
    "            \"[%s]\" % re.escape(string.punctuation), \"\", text\n",
    "        )  # Eliminar signos de puntuacion\n",
    "        # text = re.sub(r\"\\w*\\d\\w*\", \"\", text)  # Eliminar numeros y palabras con numeros\n",
    "        text = re.sub(r\"[^A-Za-z\\s]*\", \"\", text)\n",
    "        # text = correct_spellings(text)  # Corregir ortografia de palabras\n",
    "        # text = delete_stopwords(text)  # Eliminar palabas comunes\n",
    "        # text = lemmatizer(text)  # Convertir las palabras a su verbo base\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data_part):\n",
    "    text = data_part[\"text\"].apply(clean_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jared/micromamba/envs/delfin/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "partitions = np.array_split(df, 16)\n",
    "pool = Pool(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parts = pool.map(process, partitions)\n",
    "df[\"text\"] = pd.concat(df_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58926"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "with open(\"./input/glove.840B.300d.pkl\", \"rb\") as f:\n",
    "    embedding_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\n",
    "def build_vocab(sentences: list) -> dict[str, float]:\n",
    "    vocab = {}\n",
    "    for text in sentences:\n",
    "        for word in text.split():\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def check_coverage(\n",
    "    vocab: dict[str, float], embeddings_index: dict[str, float]\n",
    ") -> tuple[float, float, list[tuple]]:\n",
    "    covered_words = {}\n",
    "    oov = {}\n",
    "    n_covered = n_oov = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered_words[word] = embeddings_index[word]\n",
    "            n_covered += vocab[word]\n",
    "        except KeyError:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "    vocab_coverage = len(covered_words) / (len(vocab))\n",
    "    text_coverage = n_covered / (n_covered + n_oov)\n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return (vocab_coverage, text_coverage, sorted_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(list(df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_coverage, text_coverage, oov = check_coverage(vocab, embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 31.90% of vocab\n",
      "Found embeddings for  97.92% of all text\n"
     ]
    }
   ],
   "source": [
    "print(\"Found embeddings for {:.2%} of vocab\".format(vocab_coverage))\n",
    "print(\"Found embeddings for  {:.2%} of all text\".format(text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker(distance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame({\"word\": list(dict(oov).keys())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spellings(word: str):\n",
    "    correction = spell.correction(word)\n",
    "    if correction:\n",
    "        return correction\n",
    "    return word\n",
    "\n",
    "\n",
    "def task(data_part):\n",
    "    text = data_part[\"word\"].apply(correct_spellings)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jared/micromamba/envs/delfin/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "partitions = np.array_split(words_df, 16)\n",
    "pool = Pool(16)\n",
    "df_parts = pool.map(task, partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.concat(df_parts)\n",
    "new_words = {}\n",
    "for i in range(len(oov)):\n",
    "    if oov[i][0] != words_df.loc[i]:\n",
    "        new_words[oov[i][0]] = words_df.loc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_word(text: str):\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        try:\n",
    "            words[i] = new_words[words[i]]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(correct_word)  # type: ignore\n",
    "vocab = build_vocab(list(df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 36.35% of vocab\n",
      "Found embeddings for  98.29% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab_coverage, text_coverage, oov = check_coverage(vocab, embedding_dict)\n",
    "print(\"Found embeddings for {:.2%} of vocab\".format(vocab_coverage))\n",
    "print(\"Found embeddings for  {:.2%} of all text\".format(text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3837"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(texts: list, embeddings_dim: int = 300):\n",
    "    text_vector = []\n",
    "    for text in texts:\n",
    "        word_vectors = []\n",
    "        for word in text.split():\n",
    "            vector = embedding_dict.get(word)\n",
    "            if vector is not None:\n",
    "                word_vectors.append(vector)\n",
    "        if len(word_vectors) == 0:\n",
    "            text_vector.append(np.zeros(embeddings_dim))\n",
    "        else:\n",
    "            text_vector.append(np.mean(word_vectors, axis=0))\n",
    "    return np.array(text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Esto es un texto\", \"This is a text\", \"i would like play video games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_v = text_to_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_to_vector(list(df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"target\"].replace(4, 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7536737390938439\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text: str):\n",
    "    class_name = [\"Negativo\", \"Positivo\"]\n",
    "    return class_name[svm.predict(text_to_vector([text]))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positivo'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"i love my car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negativo'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\n",
    "    \"i like walking with my dog ​​in the park but I do not like walking when it rains so I'm sad today because it is raining and I can not go for a walk in the park\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negativo'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positivo\n",
    "predict(\n",
    "    \"i do not like going to the cinema i like watching movies at home because there are no annoying noises and i really like silence\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positivo'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\n",
    "    \"i do not like going to the cinema i like watching movies at home today i will watch my favorite movie\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
