{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 17:11:12.529246: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow_datasets as tfds\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import contractions\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, TextVectorization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spellchecker import SpellChecker\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "# tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About Sentiment140\n",
    "\n",
    "This is the sentiment140 dataset.\n",
    "\n",
    "It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 2 = neutral, 4 = positive) and they can be used to detect sentiment .\n",
    "\n",
    "It contains the following 6 fields:\n",
    "\n",
    "1. target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "2. ids: The id of the tweet ( 2087)\n",
    "3. date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "4. flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "5. user: the user that tweeted (robotickilldozr)\n",
    "6. text: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar datos desde los datasets de tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = tfds.load(\"sentiment140\", data_dir=\"./input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = pd.read_csv(\"./input/sentiment140.csv\", encoding=\"ISO-8859-1\", names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminacion de las columnas que no son necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop([\"id\", \"date\", \"flag\", \"user\"], axis=\"columns\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos no hay valores faltantes, pero si hay algunas filas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df: DataFrame):\n",
    "    print(\"Shape: \", df.shape)\n",
    "    print(\"Duplicate rows: \", df.duplicated().sum())\n",
    "    return pd.DataFrame(\n",
    "        index=df.columns,\n",
    "        data={\n",
    "            \"Unique\": df.nunique().values,\n",
    "            \"Missing\": df.isnull().sum().values,\n",
    "            \"Type\": df.dtypes,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (1600000, 2)\n",
      "Duplicate rows:  16309\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique</th>\n",
       "      <th>Missing</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>1581466</td>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unique  Missing    Type\n",
       "target        2        0   int64\n",
       "text    1581466        0  object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminacion de filas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      "@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
      "my whole body feels itchy and like its on fire \n",
      "@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \n",
      "@Kwesidei not the whole crew \n",
      "Need a hug \n",
      "@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\n",
      "@Tatiana_K nope they didn't have it \n",
      "@twittera que me muera ? \n",
      "spring break in plain city... it's snowing \n",
      "I just re-pierced my ears \n",
      "@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .\n",
      "@octolinz16 It it counts, idk why I did either. you never talk to me anymore \n",
      "@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\n",
      "@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!\n",
      "Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\n",
      "about to file taxes \n",
      "@LettyA ahh ive always wanted to see rent  love the soundtrack!!\n",
      "@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks? \n",
      "@alydesigns i was out most of the day so didn't get much done \n",
      "one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh* \n",
      "@angry_barista I baked you a cake but I ated it \n",
      "this week is not going as i had hoped \n",
      "blagh class at 8 tomorrow \n",
      "I hate when I have to call and wake people up \n",
      "Just going to cry myself to sleep after watching Marley and Me.  \n",
      "im sad now  Miss.Lilly\n",
      "ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again \n",
      "Meh... Almost Lover is the exception... this track gets me depressed every time. \n",
      "some1 hacked my account on aim  now i have to make a new one\n",
      "@alielayus I want to go to promote GEAR AND GROOVE but unfornately no ride there  I may b going to the one in Anaheim in May though\n",
      "thought sleeping in was an option tomorrow but realizing that it now is not. evaluations in the morning and work in the afternoon! \n",
      "@julieebaby awe i love you too!!!! 1 am here  i miss you\n",
      "@HumpNinja I cry my asian eyes to sleep at night \n",
      "ok I'm sick and spent an hour sitting in the shower cause I was too sick to stand and held back the puke like a champ. BED now \n",
      "@cocomix04 ill tell ya the story later  not a good day and ill be workin for like three more hours...\n",
      "@MissXu sorry! bed time came here (GMT+1)   http://is.gd/fNge\n",
      "@fleurylis I don't either. Its depressing. I don't think I even want to know about the kids in suitcases. \n",
      "Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend \n",
      "really don't feel like getting up today... but got to study to for tomorrows practical exam... \n",
      "He's the reason for the teardrops on my guitar the only one who has enough of me to break my heart \n",
      "Sad, sad, sad. I don't know why but I hate this feeling  I wanna sleep and I still can't!\n",
      "@JonathanRKnight Awww I soo wish I was there to see you finally comfortable! Im sad that I missed it \n",
      "Falling asleep. Just heard about that Tracy girl's body being found. How sad  My heart breaks for that family.\n",
      "@Viennah Yay! I'm happy for you with your job! But that also means less time for me and you... \n",
      "Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?\n",
      "Oh man...was ironing @jeancjumbe's fave top to wear to a meeting. Burnt it \n",
      "is strangely sad about LiLo and SamRo breaking up. \n",
      "@tea oh! i'm so sorry  i didn't think about that before retweeting.\n",
      "Broadband plan 'a massive broken promise' http://tinyurl.com/dcuc33 via www.diigo.com/~tautao Still waiting for broadband we are \n",
      "@localtweeps Wow, tons of replies from you, may have to unfollow so I can see my friends' tweets, you're scrolling the feed a lot. \n",
      "our duck and chicken are taking wayyy too long to hatch \n",
      "Put vacation photos online a few yrs ago. PC crashed, and now I forget the name of the site. \n",
      "I need a hug \n",
      "@andywana Not sure what they are, only that they are PoS! As much as I want to, I dont think can trade away company assets sorry andy! \n",
      "@oanhLove I hate when that happens... \n",
      "I have a sad feeling that Dallas is not going to show up  I gotta say though, you'd think more shows would use music from the game. mmm\n",
      "Ugh....92 degrees tomorrow \n",
      "Where did u move to?  I thought u were already in sd. ?? Hmmm. Random u found me. Glad to hear yer doing well.\n",
      "@BatManYNG I miss my ps3, it's out of commission  Wutcha playing? Have you copped 'Blood On The Sand'?\n",
      "just leaving the parking lot of work! \n",
      "The Life is cool. But not for Me. \n",
      "Sadly though, I've never gotten to experience the post coitus cigarette before, and now I never will. \n",
      "I had such a nice day. Too bad the rain comes in tomorrow at 5am \n",
      "@Starrbby too bad I won't be around I lost my job and can't even pay my phone bill lmao aw shucks \n",
      "Damm back to school tomorrow \n",
      "Mo jobs, no money.  how in the hell is min wage here 4 f'n clams an hour?\n",
      "@katortiz  Not forever... See you soon!\n",
      "@Lt_Algonquin agreed, I saw the failwhale allllll day today. \n",
      "@jdarter Oh! Haha... dude I dont really look at em unless someone says HEY I ADDED YOU. Sorry  I'm so terrible at that. I need a pop up!\n",
      "@ninjen I'm sure you're right...    I need to start working out with you and the Nikster... Or Jared at least!\n",
      "i really hate how people diss my bands!  Trace is clearly NOT ugly!\n",
      "Gym attire today was: Puma singlet, Adidas shorts.......and black business socks and leather shoes  Lucky did not run into any cute girls.\n",
      "Why won't you show my location?!   http://twitpic.com/2y2es\n",
      "No picnic  my phone smells like citrus.\n",
      "@ashleyac My donkey is sensitive about such comments. Nevertheless, he'd (and me'd) be glad to see your mug asap. Charger is still awol. \n",
      "No new csi tonight.  FML\n",
      "i think my arms are sore from tennis \n",
      "wonders why someone that u like so much can make you so unhappy in a split seccond . depressed . \n",
      "sleep soon... i just hate saying bye and see you tomorrow for the night. \n",
      "@statravelAU just got ur newsletter, those fares really are unbelievable, shame I already booked and paid for mine \n",
      "missin' the boo \n",
      "@markhardy1974 Me too  #itm\n",
      "Damn... I don't have any chalk! MY CHALKBOARD IS USELESS \n",
      "had a blast at the Getty Villa, but hates that she's had a sore throat all day. It's just getting worse too \n",
      "@msdrama hey missed ya at the meeting  sup mama\n",
      "My tummy hurts.  I wonder if the hypnosis has anything to do with it? If so, it's working, I get it, STOP SMOKING!!!\n",
      "why is it always the fat ones?! \n",
      "@januarycrimson Sorry, babe!!  My fam annoys me too. Thankfully, they're asleep right now. Muahaha. *evil laugh*\n",
      "@Hollywoodheat I should have paid more attention when we covered photoshop in my webpage design class in undergrad \n",
      "wednesday my b-day! don't know what 2 do!! \n",
      "Poor cameron (the hills) \n",
      "pray for me please, the ex is threatening to start sh** at my/our babies 1st Birthday party. what a jerk. and I still have a headache \n",
      "@makeherfamous hmm  , do u really enjoy being with him ? if the problems are too constants u should think things more , find someone ulike\n",
      "Strider is a sick little puppy  http://apps.facebook.com/dogbook/profile/view/5248435\n",
      "so rylee,grace...wana go steve's party or not?? SADLY SINCE ITS EASTER I WNT B ABLE 2 DO MUCH  BUT OHH WELL.....\n",
      "hey, I actually won one of my bracket pools! Too bad it wasn't the one for money \n",
      "@stark YOU don't follow me, either  and i work for you!\n",
      "A bad nite for the favorite teams: Astros and Spartans lose.  The nite out with T.W. was good.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(df[\"text\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializacion de variables a usar en la limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jared/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jared/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell = SpellChecker()\n",
    "wln = WordNetLemmatizer()\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spellings(text: str) -> str:\n",
    "    words = text.split()\n",
    "    misspelled = spell.unknown(words)\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in misspelled:\n",
    "            word = spell.correction(words[i])\n",
    "            if word:\n",
    "                words[i] = word  # type: ignore\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'correct is you running'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_spellings(\"corect is you runing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(text: str):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'would first gun really though zap spders jus doucheclown'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_stopwords(\n",
    "    \"i would have been the first but i did not have a gun not really though zap spders jus a doucheclown\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text: str):\n",
    "    words = [wln.lemmatize(word, pos=\"v\") for word in text.split()]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"be upset that he can't update his Facebook by texting it and might cry as a result School today also. Blah!\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\n",
    "    \"is upset that he can't update his Facebook by texting it and might cry as a result  School today also. Blah!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    if not text == \"\":\n",
    "        text = text.lower()  # Convertir minusculas todo el texto\n",
    "        text = re.sub(\n",
    "            r\"@[\\S]+\", \"\", text\n",
    "        )  # Eliminar los nombres de usuarios con @ mencionados\n",
    "        text = re.sub(\n",
    "            r\"((www\\.[\\S]+)|([https]+://[\\S]+))\", \"\", text\n",
    "        )  # Eliminar las urls mencionadas\n",
    "        text = re.sub(\n",
    "            r\"^\\s+|\\s+$|\\s+(?=\\s)\", \"\", text\n",
    "        )  # Eliminar espacios en blanco extras\n",
    "        text = contractions.fix(text)  # type: ignore # Expandir las contracciones\n",
    "\n",
    "        text = re.sub(\n",
    "            \"[%s]\" % re.escape(string.punctuation), \"\", text\n",
    "        )  # Eliminar signos de puntuacion\n",
    "        # text = re.sub(r\"\\w*\\d\\w*\", \"\", text)  # Eliminar numeros y palabras con numeros\n",
    "        text = re.sub(r\"[^A-Za-z\\s]*\", \"\", text)\n",
    "        # text = correct_spellings(text)  # Corregir ortografia de palabras\n",
    "        # text = delete_stopwords(text)  # Eliminar palabas comunes\n",
    "        # text = lemmatizer(text)  # Convertir las palabras a su verbo base\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data_part):\n",
    "    text = data_part[\"text\"].apply(clean_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = np.array_split(df, 16)\n",
    "pool = Pool(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parts = pool.map(process, partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.concat(df_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./input/glove.840B.300d.txt\"\n",
    "\n",
    "\n",
    "embedding_dict = {}\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split(\" \")\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:], \"float32\")\n",
    "        embedding_dict[word] = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./input/glove.840B.300d.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embedding_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./input/glove.840B.300d.pkl\", \"rb\") as f:\n",
    "    embedding_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\n",
    "def build_vocab(sentences: list) -> dict[str, float]:\n",
    "    vocab = {}\n",
    "    for text in sentences:\n",
    "        for word in text.split():\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def check_coverage(\n",
    "    vocab: dict[str, float], embeddings_index: dict[str, float]\n",
    ") -> tuple[float, float, list[tuple]]:\n",
    "    covered_words = {}\n",
    "    oov = {}\n",
    "    n_covered = n_oov = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered_words[word] = embeddings_index[word]\n",
    "            n_covered += vocab[word]\n",
    "        except KeyError:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "    vocab_coverage = len(covered_words) / (len(vocab))\n",
    "    text_coverage = n_covered / (n_covered + n_oov)\n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return (vocab_coverage, text_coverage, sorted_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(list(clean_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'awww': 4980,\n",
       " 'that': 172038,\n",
       " 'is': 383256,\n",
       " 'a': 376520,\n",
       " 'bummer': 1442,\n",
       " 'you': 364888,\n",
       " 'shoulda': 348,\n",
       " 'got': 69383,\n",
       " 'david': 2267,\n",
       " 'carr': 75,\n",
       " 'of': 189222,\n",
       " 'third': 814,\n",
       " 'day': 84050,\n",
       " 'to': 614198,\n",
       " 'do': 136474,\n",
       " 'it': 298478,\n",
       " 'd': 6757,\n",
       " 'upset': 2641,\n",
       " 'he': 37483,\n",
       " 'cannot': 64692,\n",
       " 'update': 4243,\n",
       " 'his': 15376,\n",
       " 'facebook': 4196,\n",
       " 'by': 30105,\n",
       " 'texting': 727,\n",
       " 'and': 297529,\n",
       " 'might': 9620,\n",
       " 'cry': 4395,\n",
       " 'as': 40616,\n",
       " 'result': 594,\n",
       " 'school': 19597,\n",
       " 'today': 64280,\n",
       " 'also': 10252,\n",
       " 'blah': 1471,\n",
       " 'i': 984284,\n",
       " 'dived': 3,\n",
       " 'many': 8816,\n",
       " 'times': 7664,\n",
       " 'for': 214795,\n",
       " 'the': 523491,\n",
       " 'ball': 1272,\n",
       " 'managed': 976,\n",
       " 'save': 2190,\n",
       " 'rest': 5203,\n",
       " 'go': 72480,\n",
       " 'out': 80092,\n",
       " 'bounds': 19,\n",
       " 'my': 311949,\n",
       " 'whole': 5949,\n",
       " 'body': 2467,\n",
       " 'feels': 5120,\n",
       " 'itchy': 440,\n",
       " 'like': 77539,\n",
       " 'its': 43693,\n",
       " 'on': 166169,\n",
       " 'fire': 1503,\n",
       " 'no': 73713,\n",
       " 'not': 281406,\n",
       " 'behaving': 68,\n",
       " 'at': 110561,\n",
       " 'all': 82820,\n",
       " 'am': 240021,\n",
       " 'mad': 3793,\n",
       " 'why': 27640,\n",
       " 'here': 36719,\n",
       " 'because': 21272,\n",
       " 'see': 45830,\n",
       " 'over': 22994,\n",
       " 'there': 45621,\n",
       " 'crew': 649,\n",
       " 'need': 35245,\n",
       " 'hug': 2102,\n",
       " 'hey': 18417,\n",
       " 'long': 16679,\n",
       " 'time': 55787,\n",
       " 'yes': 17891,\n",
       " 'rains': 546,\n",
       " 'bit': 10890,\n",
       " 'only': 27525,\n",
       " 'lol': 55129,\n",
       " 'fine': 4409,\n",
       " 'thanks': 41796,\n",
       " 'how': 43974,\n",
       " 'nope': 2534,\n",
       " 'they': 42718,\n",
       " 'did': 52487,\n",
       " 'have': 184712,\n",
       " 'que': 564,\n",
       " 'me': 159781,\n",
       " 'muera': 1,\n",
       " 'spring': 961,\n",
       " 'break': 4381,\n",
       " 'in': 213701,\n",
       " 'plain': 407,\n",
       " 'city': 3637,\n",
       " 'snowing': 144,\n",
       " 'just': 124499,\n",
       " 'repierced': 15,\n",
       " 'ears': 823,\n",
       " 'could': 28181,\n",
       " 'bear': 826,\n",
       " 'watch': 15445,\n",
       " 'thought': 10928,\n",
       " 'ua': 19,\n",
       " 'loss': 1012,\n",
       " 'was': 110751,\n",
       " 'embarrassing': 191,\n",
       " 'counts': 232,\n",
       " 'know': 55252,\n",
       " 'either': 5206,\n",
       " 'never': 17552,\n",
       " 'talk': 8055,\n",
       " 'anymore': 6456,\n",
       " 'would': 42516,\n",
       " 'been': 33393,\n",
       " 'first': 16498,\n",
       " 'but': 127238,\n",
       " 'gun': 291,\n",
       " 'really': 49675,\n",
       " 'though': 30999,\n",
       " 'zac': 261,\n",
       " 'snyders': 2,\n",
       " 'doucheclown': 1,\n",
       " 'wish': 27526,\n",
       " 'with': 114428,\n",
       " 'miss': 35718,\n",
       " 'premiere': 362,\n",
       " 'hollis': 6,\n",
       " 'death': 1678,\n",
       " 'scene': 673,\n",
       " 'will': 105729,\n",
       " 'hurt': 4992,\n",
       " 'severely': 69,\n",
       " 'film': 1887,\n",
       " 'wry': 11,\n",
       " 'directors': 43,\n",
       " 'cut': 3484,\n",
       " 'now': 90285,\n",
       " 'about': 52451,\n",
       " 'file': 513,\n",
       " 'taxes': 133,\n",
       " 'ahh': 3044,\n",
       " 'always': 14714,\n",
       " 'wanted': 7502,\n",
       " 'rent': 533,\n",
       " 'love': 65386,\n",
       " 'soundtrack': 303,\n",
       " 'oh': 38043,\n",
       " 'dear': 3566,\n",
       " 'were': 18822,\n",
       " 'drinking': 2805,\n",
       " 'forgotten': 513,\n",
       " 'table': 825,\n",
       " 'drinks': 1110,\n",
       " 'most': 7718,\n",
       " 'so': 149578,\n",
       " 'get': 81282,\n",
       " 'much': 36410,\n",
       " 'done': 14960,\n",
       " 'one': 52001,\n",
       " 'friend': 10347,\n",
       " 'called': 4195,\n",
       " 'asked': 1822,\n",
       " 'meet': 5283,\n",
       " 'her': 30534,\n",
       " 'mid': 309,\n",
       " 'valley': 322,\n",
       " 'todaybut': 92,\n",
       " 'sigh': 3263,\n",
       " 'baked': 365,\n",
       " 'cake': 2356,\n",
       " 'ated': 3,\n",
       " 'this': 92673,\n",
       " 'week': 20843,\n",
       " 'going': 88578,\n",
       " 'had': 48264,\n",
       " 'hoped': 278,\n",
       " 'blagh': 8,\n",
       " 'class': 5760,\n",
       " 'tomorrow': 33606,\n",
       " 'hate': 19420,\n",
       " 'when': 38932,\n",
       " 'call': 8211,\n",
       " 'wake': 4321,\n",
       " 'people': 23225,\n",
       " 'up': 88133,\n",
       " 'myself': 8879,\n",
       " 'sleep': 26743,\n",
       " 'after': 18601,\n",
       " 'watching': 22274,\n",
       " 'marley': 337,\n",
       " 'sad': 28478,\n",
       " 'misslilly': 1,\n",
       " 'ooooh': 394,\n",
       " 'leslie': 83,\n",
       " 'ok': 15259,\n",
       " 'again': 29470,\n",
       " 'meh': 740,\n",
       " 'almost': 7698,\n",
       " 'lover': 349,\n",
       " 'exception': 117,\n",
       " 'track': 1281,\n",
       " 'gets': 4322,\n",
       " 'depressed': 1321,\n",
       " 'every': 6916,\n",
       " 'some': 45465,\n",
       " 'hacked': 342,\n",
       " 'account': 2936,\n",
       " 'aim': 713,\n",
       " 'make': 24463,\n",
       " 'new': 41769,\n",
       " 'want': 58011,\n",
       " 'promote': 208,\n",
       " 'gear': 504,\n",
       " 'groove': 102,\n",
       " 'unfornately': 2,\n",
       " 'ride': 3038,\n",
       " 'may': 8091,\n",
       " 'b': 5215,\n",
       " 'anaheim': 64,\n",
       " 'sleeping': 3963,\n",
       " 'an': 34881,\n",
       " 'option': 655,\n",
       " 'realizing': 222,\n",
       " 'evaluations': 9,\n",
       " 'morning': 32200,\n",
       " 'work': 61561,\n",
       " 'afternoon': 3918,\n",
       " 'awe': 772,\n",
       " 'too': 64071,\n",
       " 'asian': 411,\n",
       " 'eyes': 3404,\n",
       " 'night': 41250,\n",
       " 'sick': 15371,\n",
       " 'spent': 2652,\n",
       " 'hour': 7471,\n",
       " 'sitting': 5229,\n",
       " 'shower': 3875,\n",
       " 'stand': 1378,\n",
       " 'held': 319,\n",
       " 'back': 55432,\n",
       " 'puke': 192,\n",
       " 'champ': 108,\n",
       " 'bed': 21136,\n",
       " 'ill': 6700,\n",
       " 'tell': 10233,\n",
       " 'ya': 9974,\n",
       " 'story': 3081,\n",
       " 'later': 9878,\n",
       " 'good': 88252,\n",
       " 'be': 112749,\n",
       " 'workin': 950,\n",
       " 'three': 2949,\n",
       " 'more': 37346,\n",
       " 'hours': 13129,\n",
       " 'sorry': 25170,\n",
       " 'came': 5932,\n",
       " 'gmt': 64,\n",
       " 'depressing': 1235,\n",
       " 'think': 40906,\n",
       " 'even': 18810,\n",
       " 'kids': 5789,\n",
       " 'suitcases': 25,\n",
       " 'gym': 3242,\n",
       " 'or': 30865,\n",
       " 'then': 33487,\n",
       " 'another': 14276,\n",
       " 'fly': 1778,\n",
       " 'girlfriend': 905,\n",
       " 'feel': 29713,\n",
       " 'getting': 23924,\n",
       " 'study': 3919,\n",
       " 'tomorrows': 661,\n",
       " 'practical': 121,\n",
       " 'exam': 5666,\n",
       " 'reason': 3393,\n",
       " 'teardrops': 31,\n",
       " 'guitar': 1860,\n",
       " 'who': 18199,\n",
       " 'has': 34293,\n",
       " 'enough': 6679,\n",
       " 'heart': 4712,\n",
       " 'feeling': 14628,\n",
       " 'still': 42649,\n",
       " 'soo': 5114,\n",
       " 'finally': 11623,\n",
       " 'comfortable': 291,\n",
       " 'missed': 10841,\n",
       " 'falling': 1355,\n",
       " 'asleep': 4103,\n",
       " 'heard': 4894,\n",
       " 'tracy': 93,\n",
       " 'girls': 5227,\n",
       " 'being': 15809,\n",
       " 'found': 8521,\n",
       " 'breaks': 542,\n",
       " 'family': 7367,\n",
       " 'yay': 13198,\n",
       " 'happy': 25998,\n",
       " 'your': 64585,\n",
       " 'job': 8328,\n",
       " 'means': 3673,\n",
       " 'less': 3225,\n",
       " 'checked': 1262,\n",
       " 'user': 453,\n",
       " 'timeline': 164,\n",
       " 'blackberry': 1510,\n",
       " 'looks': 10244,\n",
       " 'twanking': 1,\n",
       " 'happening': 1245,\n",
       " 'are': 129683,\n",
       " 'having': 15941,\n",
       " 'probs': 616,\n",
       " 'w': 6590,\n",
       " 'bgs': 25,\n",
       " 'uids': 1,\n",
       " 'manwas': 1,\n",
       " 'ironing': 277,\n",
       " 'fave': 1134,\n",
       " 'top': 3490,\n",
       " 'wear': 2469,\n",
       " 'meeting': 3318,\n",
       " 'burnt': 1438,\n",
       " 'strangely': 113,\n",
       " 'lilo': 22,\n",
       " 'samro': 1,\n",
       " 'breaking': 1024,\n",
       " 'before': 14074,\n",
       " 'retweeting': 175,\n",
       " 'broadband': 209,\n",
       " 'plan': 2794,\n",
       " 'massive': 835,\n",
       " 'broken': 3000,\n",
       " 'promise': 1241,\n",
       " 'via': 2973,\n",
       " 'waiting': 9563,\n",
       " 'we': 62291,\n",
       " 'wow': 10353,\n",
       " 'tons': 805,\n",
       " 'replies': 675,\n",
       " 'from': 55807,\n",
       " 'unfollow': 381,\n",
       " 'can': 46728,\n",
       " 'friends': 14559,\n",
       " 'tweets': 6118,\n",
       " 'scrolling': 51,\n",
       " 'feed': 982,\n",
       " 'lot': 9021,\n",
       " 'our': 15076,\n",
       " 'duck': 310,\n",
       " 'chicken': 2126,\n",
       " 'taking': 5841,\n",
       " 'wayyy': 199,\n",
       " 'hatch': 31,\n",
       " 'put': 8130,\n",
       " 'vacation': 2430,\n",
       " 'photos': 2322,\n",
       " 'online': 4445,\n",
       " 'few': 9166,\n",
       " 'yrs': 904,\n",
       " 'ago': 5210,\n",
       " 'pc': 1615,\n",
       " 'crashed': 806,\n",
       " 'forget': 2890,\n",
       " 'name': 5477,\n",
       " 'site': 4012,\n",
       " 'sure': 14699,\n",
       " 'what': 60466,\n",
       " 'pos': 61,\n",
       " 'trade': 388,\n",
       " 'away': 11911,\n",
       " 'company': 1838,\n",
       " 'assets': 29,\n",
       " 'andy': 810,\n",
       " 'happens': 2001,\n",
       " 'dallas': 680,\n",
       " 'show': 15816,\n",
       " 'say': 16047,\n",
       " 'shows': 2497,\n",
       " 'use': 7731,\n",
       " 'music': 8497,\n",
       " 'game': 9295,\n",
       " 'mmm': 1389,\n",
       " 'ugh': 9160,\n",
       " 'degrees': 975,\n",
       " 'where': 14866,\n",
       " 'move': 3756,\n",
       " 'already': 14449,\n",
       " 'sd': 313,\n",
       " 'hmmm': 1798,\n",
       " 'random': 1848,\n",
       " 'glad': 10375,\n",
       " 'hear': 9948,\n",
       " 'yer': 420,\n",
       " 'doing': 16025,\n",
       " 'well': 40708,\n",
       " 'ps': 3156,\n",
       " 'commission': 96,\n",
       " 'wutcha': 4,\n",
       " 'playing': 7196,\n",
       " 'copped': 24,\n",
       " 'blood': 1616,\n",
       " 'sand': 297,\n",
       " 'leaving': 5233,\n",
       " 'parking': 560,\n",
       " 'life': 14993,\n",
       " 'cool': 13922,\n",
       " 'sadly': 2382,\n",
       " 'gotten': 1243,\n",
       " 'experience': 1199,\n",
       " 'post': 5258,\n",
       " 'coitus': 1,\n",
       " 'cigarette': 144,\n",
       " 'such': 9075,\n",
       " 'nice': 23037,\n",
       " 'bad': 26593,\n",
       " 'rain': 10332,\n",
       " 'comes': 3514,\n",
       " 'around': 8881,\n",
       " 'lost': 11171,\n",
       " 'pay': 4096,\n",
       " 'phone': 12981,\n",
       " 'bill': 1030,\n",
       " 'lmao': 4114,\n",
       " 'aw': 4317,\n",
       " 'shucks': 195,\n",
       " 'damm': 110,\n",
       " 'mo': 983,\n",
       " 'jobs': 776,\n",
       " 'money': 6089,\n",
       " 'hell': 5283,\n",
       " 'min': 1747,\n",
       " 'wage': 41,\n",
       " 'fn': 88,\n",
       " 'clams': 14,\n",
       " 'forever': 2467,\n",
       " 'soon': 18138,\n",
       " 'agreed': 712,\n",
       " 'saw': 9817,\n",
       " 'failwhale': 20,\n",
       " 'allllll': 90,\n",
       " 'haha': 30261,\n",
       " 'dude': 4119,\n",
       " 'look': 13079,\n",
       " 'them': 27051,\n",
       " 'unless': 1226,\n",
       " 'someone': 11462,\n",
       " 'says': 5890,\n",
       " 'added': 1201,\n",
       " 'terrible': 1867,\n",
       " 'pop': 1053,\n",
       " 'right': 27377,\n",
       " 'start': 10386,\n",
       " 'working': 16359,\n",
       " 'nikster': 1,\n",
       " 'jared': 217,\n",
       " 'least': 7972,\n",
       " 'diss': 54,\n",
       " 'bands': 589,\n",
       " 'trace': 167,\n",
       " 'clearly': 555,\n",
       " 'ugly': 1024,\n",
       " 'attire': 40,\n",
       " 'puma': 31,\n",
       " 'singlet': 10,\n",
       " 'adidas': 39,\n",
       " 'shortsand': 1,\n",
       " 'black': 2844,\n",
       " 'business': 1961,\n",
       " 'socks': 438,\n",
       " 'leather': 196,\n",
       " 'shoes': 2181,\n",
       " 'lucky': 4152,\n",
       " 'run': 5074,\n",
       " 'into': 11284,\n",
       " 'any': 15216,\n",
       " 'cute': 7570,\n",
       " 'location': 402,\n",
       " 'picnic': 577,\n",
       " 'smells': 712,\n",
       " 'citrus': 26,\n",
       " 'donkey': 89,\n",
       " 'sensitive': 140,\n",
       " 'comments': 1019,\n",
       " 'nevertheless': 65,\n",
       " 'med': 166,\n",
       " 'mug': 224,\n",
       " 'possible': 2338,\n",
       " 'charger': 578,\n",
       " 'awol': 30,\n",
       " 'csi': 242,\n",
       " 'tonight': 24713,\n",
       " 'fml': 1713,\n",
       " 'arms': 791,\n",
       " 'sore': 3667,\n",
       " 'tennis': 811,\n",
       " 'wonders': 500,\n",
       " 'unhappy': 332,\n",
       " 'split': 369,\n",
       " 'seccond': 2,\n",
       " 'saying': 3027,\n",
       " 'bye': 3731,\n",
       " 'newsletter': 133,\n",
       " 'those': 9789,\n",
       " 'fares': 23,\n",
       " 'unbelievable': 162,\n",
       " 'shame': 2482,\n",
       " 'booked': 627,\n",
       " 'paid': 1374,\n",
       " 'mine': 6540,\n",
       " 'missin': 712,\n",
       " 'boo': 4810,\n",
       " 'itm': 9,\n",
       " 'damn': 11126,\n",
       " 'chalk': 53,\n",
       " 'chalkboard': 9,\n",
       " 'useless': 418,\n",
       " 'blast': 1282,\n",
       " 'getty': 27,\n",
       " 'villa': 59,\n",
       " 'hates': 2080,\n",
       " 'she': 30425,\n",
       " 'throat': 2760,\n",
       " 'worse': 2742,\n",
       " 'sup': 220,\n",
       " 'mama': 1079,\n",
       " 'tummy': 1928,\n",
       " 'hurts': 6798,\n",
       " 'wonder': 2762,\n",
       " 'if': 42737,\n",
       " 'hypnosis': 15,\n",
       " 'anything': 7944,\n",
       " 'stop': 7824,\n",
       " 'smoking': 454,\n",
       " 'fat': 1651,\n",
       " 'ones': 3677,\n",
       " 'babe': 2044,\n",
       " 'fam': 1537,\n",
       " 'annoys': 100,\n",
       " 'thankfully': 332,\n",
       " 'muahaha': 50,\n",
       " 'evil': 989,\n",
       " 'laugh': 2017,\n",
       " 'should': 24257,\n",
       " 'attention': 861,\n",
       " 'covered': 603,\n",
       " 'photoshop': 450,\n",
       " 'webpage': 65,\n",
       " 'design': 1225,\n",
       " 'undergrad': 29,\n",
       " 'wednesday': 2045,\n",
       " 'bday': 1076,\n",
       " 'poor': 7915,\n",
       " 'cameron': 113,\n",
       " 'hills': 1414,\n",
       " 'pray': 1446,\n",
       " 'please': 15600,\n",
       " 'ex': 624,\n",
       " 'threatening': 61,\n",
       " 'sh': 78,\n",
       " 'myour': 2,\n",
       " 'babies': 953,\n",
       " 'st': 4682,\n",
       " 'birthday': 12633,\n",
       " 'party': 9469,\n",
       " 'jerk': 358,\n",
       " 'headache': 5015,\n",
       " 'hmm': 2698,\n",
       " 'enjoy': 7335,\n",
       " 'him': 19068,\n",
       " 'problems': 1617,\n",
       " 'constants': 4,\n",
       " 'things': 11414,\n",
       " 'find': 14057,\n",
       " 'ulike': 2,\n",
       " 'strider': 2,\n",
       " 'little': 16741,\n",
       " 'puppy': 1525,\n",
       " 'ryleegracewana': 1,\n",
       " 'steves': 35,\n",
       " 'since': 9292,\n",
       " 'easter': 382,\n",
       " 'wnt': 162,\n",
       " 'able': 5420,\n",
       " 'ohh': 1767,\n",
       " 'actually': 9842,\n",
       " 'won': 3559,\n",
       " 'bracket': 34,\n",
       " 'pools': 87,\n",
       " 'follow': 11014,\n",
       " 'nite': 4063,\n",
       " 'favorite': 4063,\n",
       " 'teams': 242,\n",
       " 'astros': 38,\n",
       " 'spartans': 14,\n",
       " 'lose': 1929,\n",
       " 'tw': 59,\n",
       " 'missing': 7989,\n",
       " 'northern': 176,\n",
       " 'calif': 20,\n",
       " 'girl': 9839,\n",
       " 'police': 437,\n",
       " 'remains': 111,\n",
       " 'california': 803,\n",
       " 'hope': 32990,\n",
       " 'increase': 147,\n",
       " 'capacity': 88,\n",
       " 'fast': 3018,\n",
       " 'yesterday': 8950,\n",
       " 'pain': 4365,\n",
       " 'fail': 3650,\n",
       " 'whale': 233,\n",
       " 'behind': 1995,\n",
       " 'classes': 1513,\n",
       " 'quothousequot': 17,\n",
       " 'remember': 4926,\n",
       " 'bum': 495,\n",
       " 'leg': 1287,\n",
       " 'strikes': 131,\n",
       " 'serious': 1675,\n",
       " 'their': 8423,\n",
       " 'kinds': 334,\n",
       " 'complaints': 97,\n",
       " 'laptop': 3463,\n",
       " 'overheating': 33,\n",
       " 'recalls': 6,\n",
       " 'emily': 440,\n",
       " 'mommy': 1579,\n",
       " 'training': 1492,\n",
       " 'misses': 1250,\n",
       " 'rather': 2726,\n",
       " 'send': 5139,\n",
       " 'messages': 1180,\n",
       " 'than': 14354,\n",
       " 'rd': 1578,\n",
       " 'mixed': 430,\n",
       " 'sophmore': 17,\n",
       " 'year': 10609,\n",
       " 'overrated': 125,\n",
       " 'wondered': 206,\n",
       " 'same': 10733,\n",
       " 'thing': 14455,\n",
       " 'moscow': 71,\n",
       " 'laying': 1509,\n",
       " 'voice': 2359,\n",
       " 'sooo': 6111,\n",
       " 'killed': 1310,\n",
       " 'off': 35940,\n",
       " 'kutner': 31,\n",
       " 'house': 12549,\n",
       " 'whyyyyyyyy': 14,\n",
       " 'mea': 23,\n",
       " 'culpa': 8,\n",
       " 'sense': 1298,\n",
       " 'suicide': 179,\n",
       " 'refuse': 273,\n",
       " 'believe': 7256,\n",
       " 'happened': 3659,\n",
       " 'grind': 303,\n",
       " 'inspirational': 138,\n",
       " 'saddening': 40,\n",
       " 'cuz': 4960,\n",
       " 'yeah': 21313,\n",
       " 'wudnt': 45,\n",
       " 'chance': 2768,\n",
       " 'hanging': 2548,\n",
       " 'crooners': 5,\n",
       " 'sing': 1685,\n",
       " 'sucks': 11304,\n",
       " 'aaw': 83,\n",
       " 'bh': 57,\n",
       " 'quotmorningquot': 10,\n",
       " 'aww': 8107,\n",
       " 'beach': 5584,\n",
       " 'pissed': 1649,\n",
       " 'asbas': 1,\n",
       " 'radio': 2201,\n",
       " 'station': 1126,\n",
       " 'n': 10076,\n",
       " 'flipped': 75,\n",
       " 'upside': 302,\n",
       " 'down': 18061,\n",
       " 'head': 8220,\n",
       " 'ramen': 159,\n",
       " 'sounds': 8599,\n",
       " 'sides': 166,\n",
       " 'mention': 1433,\n",
       " 'crying': 2265,\n",
       " 'made': 13521,\n",
       " 'late': 8922,\n",
       " 'snack': 370,\n",
       " 'glass': 1117,\n",
       " 'oj': 125,\n",
       " 'bc': 937,\n",
       " 'quotdown': 10,\n",
       " 'sicknessquot': 3,\n",
       " 'sleepugh': 5,\n",
       " 'big': 10918,\n",
       " 'fan': 3884,\n",
       " 'camilla': 52,\n",
       " 'belle': 116,\n",
       " 'wah': 401,\n",
       " 'clip': 384,\n",
       " 'must': 8463,\n",
       " 'elstupido': 1,\n",
       " 'filters': 43,\n",
       " 'wait': 21816,\n",
       " 'till': 7880,\n",
       " 'puter': 76,\n",
       " 'something': 14178,\n",
       " 'else': 5164,\n",
       " 'blame': 909,\n",
       " 'broke': 4267,\n",
       " 'seems': 5028,\n",
       " 'longer': 2696,\n",
       " 'terms': 246,\n",
       " 'cold': 8115,\n",
       " 'ehhh': 110,\n",
       " 'weathers': 259,\n",
       " 'take': 16830,\n",
       " 'turn': 3128,\n",
       " 'cooooold': 14,\n",
       " 'incredible': 549,\n",
       " 'stuff': 9633,\n",
       " 'hoping': 3775,\n",
       " 'rumbles': 9,\n",
       " 'notice': 924,\n",
       " 'told': 4484,\n",
       " 'agency': 133,\n",
       " 'said': 9468,\n",
       " 'bedtime': 689,\n",
       " 'alive': 1072,\n",
       " 'yawwwnn': 1,\n",
       " 'tired': 15395,\n",
       " 'try': 10299,\n",
       " 'hopefully': 5568,\n",
       " 'headstart': 6,\n",
       " 'aghsnow': 1,\n",
       " 'kenny': 180,\n",
       " 'powers': 226,\n",
       " 'thank': 17569,\n",
       " 'letting': 1052,\n",
       " 'direct': 910,\n",
       " 'message': 2982,\n",
       " 'bridget': 74,\n",
       " 'india': 512,\n",
       " 'th': 9239,\n",
       " 'test': 3229,\n",
       " 'victory': 208,\n",
       " 'consecutive': 44,\n",
       " 'win': 6166,\n",
       " 'without': 7140,\n",
       " 'guess': 11163,\n",
       " 'stephan': 16,\n",
       " 'leavin': 313,\n",
       " 'intending': 9,\n",
       " 'finish': 3712,\n",
       " 'editing': 678,\n",
       " 'page': 3001,\n",
       " 'novel': 237,\n",
       " 'manuscript': 16,\n",
       " 'probably': 6994,\n",
       " 'happen': 3368,\n",
       " 'pages': 831,\n",
       " 'left': 11233,\n",
       " 'laid': 362,\n",
       " 'read': 8216,\n",
       " 'thampth': 1,\n",
       " 'princess': 635,\n",
       " 'diaries': 54,\n",
       " 'saving': 569,\n",
       " 'francesca': 16,\n",
       " 'end': 7294,\n",
       " 'easy': 2664,\n",
       " 'books': 2136,\n",
       " 'nokia': 319,\n",
       " 'died': 3461,\n",
       " 'mom': 7803,\n",
       " 'breast': 143,\n",
       " 'cancer': 745,\n",
       " 'worried': 1711,\n",
       " 'better': 22799,\n",
       " 'understood': 181,\n",
       " 'daylight': 105,\n",
       " 'savings': 99,\n",
       " 'ended': 1597,\n",
       " 'breakfast': 4366,\n",
       " 'keep': 11057,\n",
       " 'waking': 1971,\n",
       " 'lame': 2048,\n",
       " 'understand': 2812,\n",
       " 'heroes': 361,\n",
       " 'season': 4189,\n",
       " 'living': 2066,\n",
       " 'downtown': 754,\n",
       " 'fun': 27801,\n",
       " 'calorie': 48,\n",
       " 'wise': 383,\n",
       " 'junk': 368,\n",
       " 'food': 7328,\n",
       " 'free': 7163,\n",
       " 'ate': 3187,\n",
       " 'sour': 264,\n",
       " 'skittles': 129,\n",
       " 'ass': 4105,\n",
       " 'cherry': 408,\n",
       " 'coke': 664,\n",
       " 'man': 13079,\n",
       " 'hard': 9461,\n",
       " 'hot': 10635,\n",
       " 'tea': 3369,\n",
       " 'studying': 3168,\n",
       " 'sleeeep': 110,\n",
       " 'eyebrows': 157,\n",
       " 'waxed': 46,\n",
       " 'phantasy': 1,\n",
       " 'star': 2771,\n",
       " 'macheist': 13,\n",
       " 'apps': 857,\n",
       " 'sweet': 6971,\n",
       " 'espresso': 152,\n",
       " 'serial': 89,\n",
       " 'although': 2041,\n",
       " 'sent': 2923,\n",
       " 'picked': 1127,\n",
       " 'mich': 64,\n",
       " 'pretty': 12443,\n",
       " 'pick': 2801,\n",
       " 'way': 28738,\n",
       " 'untiltonight': 2,\n",
       " 'alone': 4881,\n",
       " 'downstairsworking': 1,\n",
       " 'anoop': 90,\n",
       " 'mean': 7456,\n",
       " 'seriously': 4277,\n",
       " 'kind': 10547,\n",
       " 'sprint': 250,\n",
       " 'g': 2177,\n",
       " 'baltimore': 150,\n",
       " 'chicago': 1312,\n",
       " 'far': 7494,\n",
       " 'stuck': 4886,\n",
       " 'awake': 4444,\n",
       " 'middle': 1473,\n",
       " 'second': 3148,\n",
       " 'row': 1158,\n",
       " 'felt': 2223,\n",
       " 'bursting': 35,\n",
       " 'bubble': 404,\n",
       " 'gosh': 1710,\n",
       " 'sooooo': 1576,\n",
       " 'until': 9294,\n",
       " 'deleted': 835,\n",
       " 'history': 1768,\n",
       " 'crazy': 5537,\n",
       " 'wind': 736,\n",
       " 'birding': 5,\n",
       " 'currently': 1762,\n",
       " 'grrr': 925,\n",
       " 'ipods': 94,\n",
       " 'acting': 813,\n",
       " 'weird': 3304,\n",
       " 'jai': 86,\n",
       " 'ho': 572,\n",
       " 'thinking': 7026,\n",
       " 'full': 5088,\n",
       " 'songs': 3346,\n",
       " 'ughh': 877,\n",
       " 'dvd': 1601,\n",
       " 'cos': 2106,\n",
       " 'heaps': 341,\n",
       " 'deal': 1772,\n",
       " 'website': 2479,\n",
       " 'through': 6575,\n",
       " 'therapyfail': 1,\n",
       " 'rail': 74,\n",
       " 'other': 10317,\n",
       " 'tips': 643,\n",
       " 'swear': 1071,\n",
       " 'losing': 1277,\n",
       " 'gaining': 89,\n",
       " 'tweeps': 1232,\n",
       " 'wrenching': 20,\n",
       " 'realized': 2013,\n",
       " 'hiding': 429,\n",
       " 'staying': 1832,\n",
       " 'househouse': 1,\n",
       " 'neighbors': 466,\n",
       " 'loudhaving': 1,\n",
       " 'danny': 1129,\n",
       " 'live': 10117,\n",
       " 'chat': 2302,\n",
       " 'car': 8593,\n",
       " 'trip': 4850,\n",
       " 'soooo': 3764,\n",
       " 'check': 8259,\n",
       " 'borders': 144,\n",
       " 'closed': 1654,\n",
       " 'downloading': 746,\n",
       " 'nins': 14,\n",
       " 'album': 3533,\n",
       " 'quotthe': 1414,\n",
       " 'slipquot': 2,\n",
       " 'come': 24626,\n",
       " 'these': 6740,\n",
       " 'days': 19165,\n",
       " 'woke': 7818,\n",
       " 'written': 605,\n",
       " 'email': 3971,\n",
       " 'early': 11407,\n",
       " 'university': 423,\n",
       " 'teach': 738,\n",
       " 'hill': 775,\n",
       " 'making': 8222,\n",
       " 'channels': 256,\n",
       " 'yet': 12867,\n",
       " 'boring': 4273,\n",
       " 'lazy': 2482,\n",
       " 'hobby': 134,\n",
       " 'buddy': 1346,\n",
       " 'ny': 1496,\n",
       " 'french': 2071,\n",
       " 'south': 1366,\n",
       " 'qtr': 13,\n",
       " 'snarl': 4,\n",
       " 'beautiful': 8278,\n",
       " 'opps': 90,\n",
       " 'remain': 164,\n",
       " 'problem': 3702,\n",
       " 'activated': 75,\n",
       " 'selfcontrol': 10,\n",
       " 'block': 1004,\n",
       " 'meaning': 484,\n",
       " 'qc': 61,\n",
       " 'regularizing': 1,\n",
       " 'internal': 120,\n",
       " 'clock': 929,\n",
       " 'difficult': 951,\n",
       " 'fb': 3195,\n",
       " 'spencer': 211,\n",
       " 'guy': 5356,\n",
       " 'reese': 42,\n",
       " 'dying': 1751,\n",
       " 'ttsc': 1,\n",
       " 'finale': 1132,\n",
       " 'next': 18373,\n",
       " 'madame': 34,\n",
       " 'president': 239,\n",
       " 'woman': 1423,\n",
       " 'limited': 312,\n",
       " 'letterstoohope': 1,\n",
       " 'guys': 13817,\n",
       " 'finei': 18,\n",
       " 'dogshes': 1,\n",
       " 'shit': 7077,\n",
       " 'screwed': 726,\n",
       " 'wanttss': 1,\n",
       " 'tonite': 1160,\n",
       " 'interview': 1818,\n",
       " 'cardiff': 145,\n",
       " 'luck': 8517,\n",
       " 'whack': 135,\n",
       " 'wiggetywhack': 1,\n",
       " 'choose': 753,\n",
       " 'chose': 272,\n",
       " 'accept': 438,\n",
       " 'familys': 101,\n",
       " 'help': 11507,\n",
       " 'dead': 3669,\n",
       " 'kill': 2158,\n",
       " 'seen': 5897,\n",
       " 'ds': 506,\n",
       " 'solid': 298,\n",
       " 'hahaha': 6619,\n",
       " 'hrs': 2379,\n",
       " 'mind': 4637,\n",
       " 'protesting': 28,\n",
       " 'quotgetting': 20,\n",
       " 'upquot': 169,\n",
       " 'nightmares': 373,\n",
       " 'boot': 399,\n",
       " 'angels': 669,\n",
       " 'library': 1132,\n",
       " 'does': 23005,\n",
       " 'nap': 3305,\n",
       " 'interrupted': 79,\n",
       " 'japanese': 575,\n",
       " 'rents': 182,\n",
       " 'longs': 24,\n",
       " 'bus': 2473,\n",
       " 'ghost': 448,\n",
       " 'world': 8728,\n",
       " 'canada': 1041,\n",
       " 'supposed': 3034,\n",
       " 'snow': 794,\n",
       " 'awwh': 129,\n",
       " 'babs': 26,\n",
       " 'underneith': 2,\n",
       " 'shop': 2239,\n",
       " 'entrance': 84,\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414983"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_coverage, text_coverage, oov = check_coverage(vocab, embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 31.90% of vocab\n",
      "Found embeddings for  97.93% of all text\n"
     ]
    }
   ],
   "source": [
    "print(\"Found embeddings for {:.2%} of vocab\".format(vocab_coverage))\n",
    "print(\"Found embeddings for  {:.2%} of all text\".format(text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quoti', 1604),\n",
       " ('gtlt', 813),\n",
       " ('quotyou', 628),\n",
       " ('youquot', 600),\n",
       " ('iranelection', 487)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker(distance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame({\"word\": list(dict(oov).keys())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spellings(word: str):\n",
    "    correction = spell.correction(word)\n",
    "    if correction:\n",
    "        return correction\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(data_part):\n",
    "    text = data_part[\"word\"].apply(correct_spellings)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jared/micromamba/envs/delfin2024/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "partitions = np.array_split(words_df, 16)\n",
    "pool = Pool(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parts = pool.map(task, partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.concat(df_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = {}\n",
    "for i in range(len(oov)):\n",
    "    if oov[i][0] != words_df.loc[i]:\n",
    "        new_words[oov[i][0]] = words_df.loc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46186"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_word(text: str):\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        try:\n",
    "            words[i] = new_words[words[i]]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.apply(correct_word)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(list(clean_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373089"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 36.35% of vocab\n",
      "Found embeddings for  98.30% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab_coverage, text_coverage, oov = check_coverage(vocab, embedding_dict)\n",
    "print(\"Found embeddings for {:.2%} of vocab\".format(vocab_coverage))\n",
    "print(\"Found embeddings for  {:.2%} of all text\".format(text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = len(vocab) - len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3572"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for text in clean_df:\n",
    "    if text == \"\":\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583691"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583691"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 17:12:07.434397: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-24 17:12:07.608835: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-24 17:12:07.609069: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-24 17:12:07.610856: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-24 17:12:07.611095: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-24 17:12:07.611283: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-24 17:12:07.680191: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-24 17:12:07.680427: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-24 17:12:07.680611: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-24 17:12:07.680890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6268 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TextVectorization(standardize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TextVectorization(standardize=None, max_tokens=max_tokens)\n",
    "tokenizer.adapt(clean_df)\n",
    "vectorized_texts = tokenizer(clean_df)\n",
    "word_index = tokenizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135620"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de embeddings: (135620, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300  # Dimensionalidad de los embeddings de GloVe\n",
    "count = 0\n",
    "# Crear una matriz de embeddings\n",
    "embedding_matrix = np.zeros((len(word_index), embedding_dim))\n",
    "for i, word in enumerate(word_index):\n",
    "    embedding_vector = embedding_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        count += 1\n",
    "\n",
    "print(f\"Matriz de embeddings: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90667"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(vectorized_texts)\n",
    "y = df[\"target\"].replace(4, 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_texts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135620"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135620, 300)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        Embedding(max_tokens, embedding_dim, weights=[embedding_matrix]),\n",
    "        Dropout(0.5),\n",
    "        LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">40,686,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │    \u001b[38;5;34m40,686,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,686,000</span> (155.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m40,686,000\u001b[0m (155.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,686,000</span> (155.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m40,686,000\u001b[0m (155.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_acc\", min_delta=1e-4, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 45ms/step - accuracy: 0.8099 - loss: 0.4146 - val_accuracy: 0.8200 - val_loss: 0.3957\n",
      "Epoch 2/8\n",
      "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 46ms/step - accuracy: 0.8222 - loss: 0.3927 - val_accuracy: 0.8245 - val_loss: 0.3893\n",
      "Epoch 3/8\n",
      "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 46ms/step - accuracy: 0.8312 - loss: 0.3753 - val_accuracy: 0.8245 - val_loss: 0.3872\n",
      "Epoch 4/8\n",
      "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 46ms/step - accuracy: 0.8381 - loss: 0.3629 - val_accuracy: 0.8262 - val_loss: 0.3865\n",
      "Epoch 5/8\n",
      "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 46ms/step - accuracy: 0.8437 - loss: 0.3518 - val_accuracy: 0.8251 - val_loss: 0.3893\n",
      "Epoch 6/8\n",
      "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 46ms/step - accuracy: 0.8490 - loss: 0.3416 - val_accuracy: 0.8251 - val_loss: 0.3930\n",
      "Epoch 7/8\n",
      "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 46ms/step - accuracy: 0.8537 - loss: 0.3317 - val_accuracy: 0.8256 - val_loss: 0.4002\n",
      "Epoch 8/8\n",
      "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 47ms/step - accuracy: 0.8572 - loss: 0.3251 - val_accuracy: 0.8241 - val_loss: 0.4055\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=1024,\n",
    "    epochs=8,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.8242 - loss: 0.4047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4053652286529541, 0.824877917766571]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text: str):\n",
    "    return \"Negativo\" if model.predict(tokenizer([text]))[0] <= 0.49 else \"Positivo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[425, 417]])>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"hello name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Negativo'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"i can not run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Negativo'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"I hate the rain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positivo'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"I love the music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positivo'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"I like pizza\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
